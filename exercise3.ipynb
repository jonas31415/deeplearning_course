{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Deep Learning for Beginners - Programming Exercises\n",
    "\n",
    "by Aline Sindel, Katharina Breininger and Tobias Würfl\n",
    "\n",
    "Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nürnberg, Erlangen, Germany \n",
    "# Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor set-up work\n",
    "import numpy as np # we will definitely need this\n",
    "\n",
    "# automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## The General Idea of the Framework\n",
    "<a id='network_description'></a>\n",
    "\n",
    "Almost all tasks in this programming exercise will revolve around implementing \"layers\". All layers are derived from the base class defined in the next cell. Each layer needs to implement the methods ```forward``` and ```backward```. We will use the term \"layer\" to represent any operator in the network that can be considered as a \"unit\" during forward and backward pass, e.g., a \"fully connected layer\", an \"activation layer\" or a \"loss layer\". \n",
    "\n",
    "In ```forward(x)```, the forward pass of the layer is computed by applying the respective operation to the input ```x```. Furthermore, intermediate results necessary to compute the gradients in the backward pass have to be stored. \n",
    "In ```backward(error)```, the layer receives the error passed down from the subsequent layer, updates its parameters accordingly and returns the error with respect to its input.\n",
    "\n",
    "This way, a simple network for classification can be expressed by a list of layer objects. Given an initial input ```x``` and a corresponding ```label```, the forward pass through the network is computed by subsequently calling ```forward``` for each layer in the list. The respective output is passed as input to the next layer. The very last layer, the \"loss\" layer, additionally receives the label to compute the loss. To adapt the weights in each layer, we then go backwards through the list, calling ```backward```, backpropagating the error through the network. The network is trained by alternating the forward and backward pass through the network while iterating through the training data.\n",
    "\n",
    "During test-time, only the forward pass through the network is computed to generate a prediction.\n",
    "\n",
    "### Basic notation and terminology\n",
    "\n",
    "We will work with the following notation and terminology:\n",
    "\n",
    "- $\\mathbf{X}$ and $\\mathbf{x}$ represent the input, \n",
    "- $\\mathbf{W}$ and $\\mathbf{w}$ the trainable weights/parameters and\n",
    "- $\\mathbf{Y}$ and $\\mathbf{y}$ the output of a layer.\n",
    "- $L$ represents the loss. Accordingly,\n",
    "- $E_\\mathbf{Y} = \\frac{\\partial L}{\\partial \\mathbf{Y}}$ is the error passed down from the subsequent layer,\n",
    "- $E_\\mathbf{W} = \\frac{\\partial L}{\\partial \\mathbf{W}}$ the error with respect to the weights and\n",
    "- $E_\\mathbf{X} = \\frac{\\partial L}{\\partial \\mathbf{X}}$ is the error with respect to the input.\n",
    "\n",
    "Note that $x$ and $y$ always have \"local\" meaning, i.e., with respect to the __current__ layer. The $y$ of the previous layer is the $x$ to the next, and vice versa.\n",
    "\n",
    "\n",
    "Have a look at the class definitions below and make yourself familiar with the concepts before continuing with the next part of the programming exercise, the fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# %load src/base.py\n",
    "def enum(*sequential, **named):\n",
    "    # Enum definition for backcompatibility\n",
    "    enums = dict(zip(sequential, range(len(sequential))), **named)\n",
    "    return type('Enum', (), enums)\n",
    "\n",
    "# Enum to encode which phase a layer is in at the moment.\n",
    "Phase = enum('train', 'test', 'validation')\n",
    "\n",
    "class BaseLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.phase = Phase.train\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the result of the forward pass of this layer. Save intermediate results\n",
    "        necessary to compute the gradients in the backward pass. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Base class - method is not implemented')\n",
    "    \n",
    "    def backward(self, error):\n",
    "        \"\"\" Update the parameters/weights of this layer (if applicable), \n",
    "        and return the gradient with respect to the input.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Base class - method is not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Fully Connected Layers\n",
    "\n",
    "Fully connected (FC) layers are the essential building blocks in (multi-layer) perceptrons. Inspired by biological neurons, they are able to represent any connection topology between two layers (without same-layer connections).\n",
    "\n",
    "<img src=\"img/ann.png\" width=\"600\">\n",
    "\n",
    "Let's have a look at the forward pass: Given an input vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ to an FC layer, the output $y$ of a single neuron can be described as a weighted sum of the input values plus a bias:\n",
    "\\begin{equation}\n",
    "y = w_{n+1} + \\sum_{j=1}^n w_j x_j ,\n",
    "\\end{equation}\n",
    "\n",
    "where we collect the weights in a vector $\\mathbf{w} \\in \\mathbb{R}^{n + 1}$.\n",
    "\n",
    "This is simply a vector-vector multiplication: \n",
    "\n",
    "\\begin{equation}\n",
    "y = \\begin{pmatrix} \n",
    "  w_{1}&\\dots&w_{n}&w_{n+1} \\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "  x_{1}    \\\\ \n",
    "  \\vdots \\\\\n",
    "  x_{n} \\\\\n",
    "  1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "By extending $\\mathbf{x}$ with an additional \"1\", we can include the bias directly in the multiplication. \n",
    "\n",
    "\n",
    "Since we want to have a layer able to generate multiple outputs, we need multiple neurons:\n",
    "\n",
    "<img src=\"img/fcn.png\" width=\"150\">\n",
    "\n",
    "To achieve this, we extend the weight vector to a matrix to allow for an output vector $\\mathbf{y} \\in \\mathbb{R}^{m}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} \n",
    "y_1    \\\\ \n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{pmatrix} &=\n",
    "\\begin{pmatrix} \n",
    "w_{1,1}    & \\dots & w_{n,1} & w_{n+1,1} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\vdots \\\\%\n",
    "w_{1,m}    & \\dots & w_{n,m} & w_{n+1,m}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "x_1    \\\\ \n",
    "\\vdots \\\\\n",
    "x_n\t \\\\\n",
    "1\n",
    "\\end{pmatrix}\\\\\n",
    "\\mathbf{y} &= \\mathbf{W}\\mathbf{x} \n",
    "\\end{align}\n",
    "\n",
    "For batch processing, we can accordingly stack multiple input vectors in a matrix $\\mathbf{X}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\mathbf{W}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "The weight matrix represents the trainable parameters of the FC layer. To be able to update the parameters, we need the gradient of the loss with respect to these weights.\n",
    "Given the error with respect to the output $\\mathbf{Y}$ of the current layer $\\frac{\\partial L}{\\partial \\mathbf{Y}} = E_\\mathbf{Y}$, we can compute the gradient with respect to the weights $\\frac{\\partial L}{\\partial \\mathbf{W}} = E_\\mathbf{W}$ using backpropagation, i.e., the chain rule. To backpropagate the error to the previous layer (and then update the weights there), we further need to compute the error with respect to the inputs $\\frac{\\partial L}{\\partial \\mathbf{X}} = E_\\mathbf{X}$.\n",
    "\n",
    "Using the formula of the fully connected layer $\\mathbf{Y} = \\mathbf{W}\\mathbf{X}$, we can compute the wanted gradients:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &= \\frac{\\partial L}{\\partial \\mathbf{Y}} \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{W}}\\\\\n",
    "                              &= E_\\mathbf{Y} \\mathbf{X}^T\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{X}} &= \\frac{\\partial L}{\\partial \\mathbf{Y}} \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{X}}\\\\\n",
    "                              &= \\mathbf{W}^T E_\\mathbf{Y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We will use (mini-batch) stochastic gradient descent in this programming exercise, so the update rule for the weights is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^{t+1} = \\mathbf{W}^{t} - \\eta E_{\\mathbf{W}^t} \\enspace{,}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is the learning rate and ${t}$ denotes the iteration.\n",
    "\n",
    "\n",
    "### Implementation task\n",
    "\n",
    "**Now it is your turn**: In the next cell, implement the methods ```init```, ```forward```, ```backward```, and ```get_gradient_weights``` and test the method by running the cell after the next. The method ```get_gradient_weights``` should return the gradient with respect to the weights and biases of the last backward pass.\n",
    "\n",
    "**Note that input and output, and accordingly the respective errors, are actually transposed compared to the formulas above**. This is due to performance reasons and consistency with known frameworks. Make sure to consider this in your implementation.\n",
    "\n",
    "Furthermore, implement the method ```initialize```. For the moment, take the initializer objects as given, we will return to them later. Just make sure to use them with the correct weight shapes to initialize weights and biases. Implement the update of these parameters as part of the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/layers/fully_connected_0.py\n",
    "#----------------------------------\n",
    "# Exercise: Fully connected layers\n",
    "#----------------------------------\n",
    "# The original python file can be reloaded by typing %load src/layers/fully_connected_0.py in the first line of this cell.\n",
    "# After successfully solving this exercise, type the following command in the first line of this cell:\n",
    "# %%writefile src/layers/fully_connected.py\n",
    "# This will save the result to a python file, which you will need for the next exercises.\n",
    "\n",
    "from src.base import BaseLayer, Phase\n",
    "\n",
    "class FullyConnectedLayer(BaseLayer):\n",
    "    def __init__(self, input_size, output_size, learning_rate):\n",
    "        \"\"\" A fully connected layer.\n",
    "            param: input_size (int): dimension n of the input vector\n",
    "            param: output_size (int): dimension m of the output vector\n",
    "            param: learning_rate (float): the learning rate of this layer\n",
    "        \"\"\"\n",
    "        # TODO: define the necessary class variables, for this have a look at the input variables and the other functions\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the foward pass through the layer.\n",
    "            param: x (np.ndarray): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (np.ndarray): result of the forward pass, of shape [b, m] where b is the batch size and\n",
    "                   m is the output size\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass of the fully connected layer\n",
    "        \n",
    "        # (1) Think about what you need to store during the forward pass to be able to compute the gradients in the backward pass \n",
    "        self.X = #TODO\n",
    "        \n",
    "        # (2) perform the actual forward pass just by matrix multiplication\n",
    "        # TODO\n",
    "        \n",
    "        # return the result        \n",
    "        pass\n",
    "    \n",
    "    def get_gradient_weights(self):\n",
    "        \"\"\" \n",
    "        returns (np.ndarray): the gradient with respect to the weights and biases from the last call of backward(...)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the getter method, hint: store the gradient in the backward pass as a class variable, \n",
    "        # then you can easily access it here.\n",
    "        pass\n",
    "    \n",
    "    def backward(self, error):\n",
    "        \"\"\" Update the weights of this layer and return the gradient with respect to the previous layer.\n",
    "            param: error (np.ndarray): of shape [b, m] where b is the batch size and m is the output size\n",
    "            returns (np.ndarray): the gradient w.r.t. the previous layer, of shape [b, n] where b is the \n",
    "                   batch size and n is the input size\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass of the fully connected layer\n",
    "        # Hint: Be careful about the order of applying the update to the weights and the calculation of \n",
    "        # the error with respect to the previous layer.\n",
    "        \n",
    "        # (1) calculate the error for lower layers using the transposed weights and the error\n",
    "        # TODO\n",
    "        \n",
    "        # (2) update own parameters\n",
    "        # TODO\n",
    "        \n",
    "        # (3) store gradient for testing purposes\n",
    "        # TODO\n",
    "        \n",
    "        # (4) update weights using learning rate and gradient\n",
    "        # TODO\n",
    "        \n",
    "        # (5) delete the bias row which has no meaning\n",
    "        \n",
    "        # TODO: return gradient w.r.t. the previous layer\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, weights_initializer, bias_initializer):\n",
    "        \"\"\" Initializes the weights/bias of this layer with the given initializers.\n",
    "            param: weights_initializer: object providing a method weights_initializer.initialize(weights_shape)\n",
    "                   which will return initialized weights with the given shape\n",
    "            param: bias_initializer: object providing a method bias_initializer.initialize(bias_shape) \n",
    "                   which will return an initialized bias with the given shape\n",
    "        \"\"\"\n",
    "        # TODO: Implement the initialization using the given initializers. Hint: Stack the weights and bias together \n",
    "        # in the weights array.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Running the testsuite\n",
    "%run Tests/TestFullyConnected.py\n",
    "TestFullyConnected.FullyConnected = FullyConnectedLayer\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Softmax and Loss Layer\n",
    "\n",
    "By combining the layers we implemented so far, we can represent a non-linear function of the input. For example, we can compute an output vector with $K$ elements to classify between $K$ classes.\n",
    "\n",
    "### Softmax\n",
    "The output of this computation is not further restricted. In many cases, however, it is beneficial if a prediction for the targeted classification has the properties of a probability distribution, i.e., \n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{k=1}^{K} y_k &= 1 \\enspace{,}\\\\\n",
    "y_k &\\le 0 \\quad \\forall k~\\text{in}~{1, ..., K} \\enspace{.}\n",
    "\\end{align}\n",
    "\n",
    "This makes it for example easier to compare the prediction with the ground truth of the classification task.\n",
    "We can achieve these properties by applying the softmax function as a last activation function. It is defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{softmax}(x_k) = \\frac{\\mathrm{exp}(x_k)}{\\sum_{j=1}^{K}\\mathrm{exp}(x_j)} \\enspace{.}\n",
    "\\end{equation}\n",
    "\n",
    "However, if the activations in $\\mathbf{x}$ are high, $\\mathrm{exp}(x_k)$ can become very large. This can cause numerical instabilities. To avoid this, the activations can be shifted by the maximum value of $\\mathbf{x}$ before applying the softmax:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\widetilde{x}} = \\mathbf{x} - \\mathrm{max}(\\mathbf{x}) \\enspace{.}\n",
    "\\end{equation}\n",
    "\n",
    "After the softmax, the predictions of the network have the properties of a probability distribution.\n",
    "\n",
    "### Loss function\n",
    "To adapt the parameters of the network, we want to know how \"well\" the network performs compared to a given ground truth (or label) - we need a loss function. Then, we can \"train\" the network by minimizing this loss by iteratively adapting the weights and biases using our training data.\n",
    "\n",
    "A common loss function is cross entropy. To compute it, we need the ground truth $\\mathbf{y^*}$ in \"one-hot\"-vector encoding. The ground truth is represented as a vector with $K$ elements where only the value that corresponds to the true class is $\\neq 0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y^*} = \n",
    "\\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  \\vdots\\\\\n",
    "  1\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Then, the cross entropy loss for a batch of b samples is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{Y^*},\\mathbf{Y}) = - \\sum_b \\sum_{k=1}^K \\ln( y_{b, k} ) y^*_{b, k}\n",
    "\\end{equation}\n",
    "\n",
    "### Combining both\n",
    "\n",
    "The softmax activation and the cross entropy loss are frequently combined, and sometimes called the \"SoftMax loss\". Together, their gradient has a simple and elegant form:\n",
    "\n",
    "\\begin{equation}\n",
    "e_k = \n",
    "y_k - y^*_k \\enspace{.}\n",
    "\\end{equation}\n",
    "\n",
    "for every element of the batch.\n",
    "\n",
    "### Implementation task\n",
    "\n",
    "Implement the softmax function and the cross entropy loss combined in the class ```SoftMaxCrossEntropyLoss```. Since the two functions are combined in ```forward```, additionally implement a function ```predict``` that computes only the softmax of the input. This function can be used during test-time, when we are interested in a prediction for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/layers/softmax_crossentropy_0.py\n",
    "#----------------------------------\n",
    "# Exercise: Softmax cross entropy\n",
    "#----------------------------------\n",
    "# The original python file can be reloaded by typing %load src/layers/softmax_crossentropy_0.py in the first line of this cell.\n",
    "# After successfully solving this exercise, type the following command in the first line of this cell:\n",
    "# %%writefile src/layers/softmax_crossentropy.py\n",
    "# This will save the result to a python file, which you will need for the next exercises.\n",
    "\n",
    "from src.base import BaseLayer, Phase\n",
    "\n",
    "class SoftMaxCrossEntropyLoss(BaseLayer):\n",
    "    def __init__(self):\n",
    "        #TODO: define class variable(s)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \"\"\" Return the cross entropy loss of the input and the labels after applying the softmax to the input. \n",
    "            param: x (np.ndarray): input, of shape [b, k] where b is the batch size and k is the input size\n",
    "            param: labels (np.ndarray): the corresponding labels of the training set in one-hot encoding for \n",
    "                   the current input, of the same shape as x\n",
    "            returns (float): the loss of the current prediction and the label\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        prediction = #TODO: use self.predict here to apply softmax to the network output\n",
    "        #Then, compute the cross entropy loss (see equation in description)\n",
    "        #Hint: for the implementation, you can simplify the equation by getting the indices for the labels beeing 1\n",
    "        pass\n",
    "    \n",
    "    def backward(self, labels):\n",
    "        \"\"\" Return the gradient of the SoftMaxCrossEntropy loss with respect to the previous layer.\n",
    "            param: labels (np.ndarray): (again) the corresponding labels of the training set for the current input, \n",
    "                   of shape [b, k] where b is the batch size and k is the input size\n",
    "            returns (np.ndarray): the error w.r.t. the previous layer, of shape [b, k] where b is the batch \n",
    "                   size and n is the input size\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass: compute the gradient of the SoftMax loss (see equation in description)\n",
    "        # For this, do not use self.prediction directly, but a copy of it\n",
    "        # Here, also you can find the positive class via the indices for the labels beeing 1\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Return the softmax of the input.  This can be interpreted as probabilistic prediction of the class.\n",
    "            param: x (np.ndarray): input with shape [b, k], where b is the batch size and n is the input size\n",
    "            returns (np.ndarray): the result softmax(x), of the same shape as x\n",
    "        \"\"\"\n",
    "        # TODO: Implement softmax (see equation in description)\n",
    "        # Hint: beforehand, shift the activation by the max value.\n",
    "        # return and store the prediction as class variable (the latter you need for the backward function)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%run Tests/TestSoftMaxCrossEntropyLoss.py\n",
    "TestSoftMaxCrossEntropyLoss.SoftMaxCrossEntropyLoss = SoftMaxCrossEntropyLoss\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d33a78639013268a0b04ee4ac4fcce955b605c5e001e96776a48262f54370188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
