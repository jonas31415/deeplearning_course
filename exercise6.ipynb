{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Deep Learning for Beginners - Programming Exercises\n",
    "\n",
    "by Aline Sindel, Katharina Breininger and Tobias Würfl\n",
    "\n",
    "Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nürnberg, Erlangen, Germany \n",
    "# Exercise 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor set-up work\n",
    "import numpy as np # we will definitely need this\n",
    "\n",
    "# automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## LeNet\n",
    "\n",
    "As the last part of the programming exercises, we use our developed operators to construct a simple neural network inspired by the traditional LeNet architecture:\n",
    "\n",
    "<figure>\n",
    "<img src=\"files/img/lenet.jpg\" width=\"600\">\n",
    "<figcaption><center>Source: LeCun et al, 1998.$^1$</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Use two convolutional layers with $5 \\times 5$ kernels and $6$ respectively $10$ channels. Each convolution is followed by a ReLU unit and max pooling with a neighborhood and stride of 2 in each dimension. The top of the network is formed by three FC layers with ReLU activations producing outputs of dimensionality $120$, $84$ and subsequently the number of categories. Finally, use the SoftMaxCrossEntropyLoss as loss layer.\n",
    "\n",
    "First, have a look at the class ```NeuralNetwork```, that provides the basic framework in which you can use the different layers and stack them together to a functioning network. You don't need to adapt this class, but you can use it to implement the LeNet architecture. You may also want to refer back to the description of the framework (Exercise 3).\n",
    "\n",
    "### Implementation task\n",
    "\n",
    "Next, implement the LeNet architecture in the ```build``` function and train and test your network using the scripts provided below. Then, choose one of the extra tasks below to tune your network.\n",
    "\n",
    "**Extra tasks**: Experiment for example with the activation function and DropOut, tune the learning rate or look at the effect of initialization. Feel free to add your own evaluations and plots. You can get the full test data of the MNIST data object by calling ```net.data_layer.get_test_set```.\n",
    "\n",
    "$^1$ LeCun Y., Bottou L., Bengio Y. and Haffner P. Gradient-based Learning Applied to Document Recognition. In Proc. IEEE, 1989."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# %load src/network.py\n",
    "\n",
    "# Nothing to do in this cell: Just make yourself familiar with the NeuralNetwork class.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, weights_initializer, bias_initializer):\n",
    "        # list which will contain the loss after training\n",
    "        self.loss = []\n",
    "        self.data_layer = None   # the layer providing data\n",
    "        self.loss_layer = None   # the layer calculating the loss and the prediction\n",
    "        self.layers = []\n",
    "        self.weights_initializer = weights_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.label_tensor = None # the labels of the current iteration\n",
    "\n",
    "    def append_fixed_layer(self, layer):\n",
    "        \"\"\" Add a non-trainable layer to the network. \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def append_trainable_layer(self, layer):\n",
    "        \"\"\" Add a new layer with trainable parameters to the network. Initialize the parameters of \n",
    "        the network using the object's initializers for weights and bias.\n",
    "        \"\"\"\n",
    "        layer.initialize(self.weights_initializer, self.bias_initializer)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Compute the forward pass through the network. \"\"\"\n",
    "        # fetch some training data\n",
    "        input_tensor, self.label_tensor = self.data_layer.forward()\n",
    "        # defer iterating through the network\n",
    "        activation_tensor = self.__forward_input(input_tensor)\n",
    "        # calculate the loss of the network using the final loss layer\n",
    "        return self.loss_layer.forward(activation_tensor, self.label_tensor)\n",
    "\n",
    "    def __forward_input(self, input_tensor):\n",
    "        \"\"\" Compute the forward pass through the network, stopping before the \n",
    "            loss layer.\n",
    "            param: input_tensor (np.ndarray): input to the network\n",
    "            returns: activation of the last \"regular\" layer\n",
    "        \"\"\"\n",
    "        activation_tensor = input_tensor\n",
    "        # pass the input up the network\n",
    "        for layer in self.layers:\n",
    "            activation_tensor = layer.forward(activation_tensor)\n",
    "        # return the activation of the last layer\n",
    "        return activation_tensor\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\" Perform the backward pass during training. \"\"\"\n",
    "        error_tensor = self.loss_layer.backward(self.label_tensor)\n",
    "        # pass back the error recursively\n",
    "        for layer in reversed(self.layers):\n",
    "            error_tensor = layer.backward(error_tensor)\n",
    "\n",
    "    def train(self, iterations):\n",
    "        \"\"\" Train the network for a fixed number of steps.\n",
    "            param: iterations (int): number of iterations for training \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.phase = Phase.train  # Make sure phase is set to \"train\" for all layers\n",
    "        for i in range(iterations):\n",
    "            loss = self.forward()  # go up the network\n",
    "            self.loss.append(loss)  # save the loss\n",
    "            self.backward()  # and down again\n",
    "            print('.', end='')\n",
    "\n",
    "\n",
    "    def test(self, input_tensor):\n",
    "        \"\"\" Apply the (trained) network to input data to generate a prediction. \n",
    "            param: input_tensor (nd.nparray): input (image or vector)\n",
    "            returns (np.ndarray): prediction by the network\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.phase = Phase.test  # Make sure phase is set to \"test\" for all layers\n",
    "        activation_tensor = self.__forward_input(input_tensor)\n",
    "        return self.loss_layer.predict(activation_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/le_net_0.py\n",
    "#----------------------------------\n",
    "# Exercise: LeNet\n",
    "#----------------------------------\n",
    "# The original python file can be reloaded by typing %load src/le_net_0.py in the first line of this cell.\n",
    "# After successfully solving this exercise, type the following command in the first line of this cell:\n",
    "# %%writefile src/le_net.py\n",
    "# This will save the result to a python file, which you will need for the next exercises.\n",
    "\n",
    "from src.network import NeuralNetwork\n",
    "## Load here your implementations of the previous exercises\n",
    "from src.layers.initializers import He, Const, UniformRandom\n",
    "from src.layers.conv import FlattenLayer, ConvolutionalLayer\n",
    "from src.layers.fully_connected import FullyConnectedLayer\n",
    "from src.layers.pooling import MaxPoolLayer\n",
    "from src.layers.activation_functions import ReLU, Sigmoid\n",
    "from src.layers.softmax_crossentropy import SoftMaxCrossEntropyLoss\n",
    "\n",
    "def build():\n",
    "    \"\"\" returns: a neural network architecture built according to the provided specification\n",
    "    \"\"\" \n",
    "    \n",
    "    net = NeuralNetwork(He(), Const(0.1))\n",
    "    learning_rate = 0.001\n",
    "    categories = 10  # MNIST, numbers 0-9\n",
    "    \n",
    "    # TODO: Implement the architecture of LeNet by adding layers to net. \n",
    "    # Have a look at the NeuralNetwork class how layers can be added.\n",
    "    # To call the constructors of the layers, have a look at your implementations of the previous exercises.\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# %load src/train_mnist.py\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = build()\n",
    "\n",
    "from Tests import Helpers\n",
    "# TODO: you can change the parameters, depending on the batch size and how many iterations you choose, the training \n",
    "# script can run for a few minutes or longer.\n",
    "\n",
    "# parameters\n",
    "batch_size = 20 #set here the batch size\n",
    "net.data_layer = Helpers.MNISTData(batch_size)\n",
    "n_iters = 100 #set here the number of iterations\n",
    "\n",
    "# training\n",
    "net.train(n_iters)\n",
    "\n",
    "# plotting\n",
    "plt.plot(range(n_iters), net.loss)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('softmax loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/test_mnist.py\n",
    "# Perform the prediction for a random test sample from the dataset:\n",
    "x, l = net.data_layer.get_random_test_sample()\n",
    "\n",
    "# plotting\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.imshow(x[:28*28].reshape(28, 28), cmap='gray')\n",
    "plt.title('Input image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# network prediction\n",
    "print('Prediction with highest output: {}'.format(np.argmax(net.test(x))))\n",
    "print('Ground truth: {}'.format(np.argmax(l)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Summary and Outlook\n",
    "In the programming exercises, we implemented some of the most common building blocks of neural networks, including fully connected layers, activation functions, convolutional layers and regularization operators. Finally, we combined these operators to working network.\n",
    "\n",
    "We covered only a small subset of elements that are relevant for neural networks. We encourage you to play with other operators, for example batch normalization$^2$, alternative activation functions, initialization strategies or recurrent units. You may also refactor the framework to experiment with different optimizers, like SGD with momentum, Adam or AdaGrad, or extend the framework to allow for weight decay.\n",
    "\n",
    "We hope you enjoyed the programming exercises and gained a deeper understanding of neural network operators and frameworks. Have fun on your journey further into deep learning and neural networks!\n",
    "\n",
    "$^2$ Ioffe S., Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proc. ICML, 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
