{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Deep Learning for Beginners - Programming Exercises\n",
    "\n",
    "by Aline Sindel, Katharina Breininger and Tobias Würfl\n",
    "\n",
    "Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nürnberg, Erlangen, Germany \n",
    "# Exercise 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor set-up work\n",
    "import numpy as np # we will definitely need this\n",
    "\n",
    "# automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Pooling Layers\n",
    "\n",
    "As alternative to striding in a convolutional layer, specific pooling layers can be used to downsample the data and condense spacial information. We will look at max pooling as one example. In the forward pass, the output for each pixel is the maximum value in a neighborhood of the corresponding input pixel, calculated separately for every channel. The downsampling is again achieved by using a stride > 1.\n",
    "\n",
    "<figure>\n",
    "<img src=\"files/img/numerical_maxpooling.gif\" width=\"400\">\n",
    "<figcaption><center>Source: https://github.com/vdumoulin/conv_arithmetic</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The above example shows maxpooling with a neighborhood of $3 \\times 3$ and a stride of $[1, 1]$.\n",
    "\n",
    "The maximum operation can be thought of as an on/off switch for the backpropagation of the gradient for each pixel. We therefore need to store the location of the maximum value in the forward pass. Since the layer has no trainable parameters, we only need to compute the gradient with respect to the input. In the backward pass, the subgradient is given by the colloquial rule \"the winner takes it all\". The error is routed only towards the maximum locations; for all other input pixels, the gradient is zero. If the stride is smaller than the neighborhood, the routed gradients for the respective pixels are summed up.\n",
    "\n",
    "### Implementation task\n",
    "\n",
    "In the following, implement the class ```MaxPoolLayer```. Check your implementation as usual by running the unittests in the cell below the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/layers/pooling_0.py\n",
    "#----------------------------------\n",
    "# Exercise: Pooling\n",
    "#----------------------------------\n",
    "# The original python file can be reloaded by typing %load src/layers/pooling_0.py in the first line of this cell.\n",
    "# After successfully solving this exercise, type the following command in the first line of this cell:\n",
    "# %%writefile src/layers/pooling.py\n",
    "# This will save the result to a python file, which you will need for the next exercises.\n",
    "\n",
    "from src.base import BaseLayer, Phase\n",
    "from src.layers.conv import FlattenLayer, ConvolutionalLayer #your results of previous exercises\n",
    "from src.layers.fully_connected import FullyConnectedLayer #your result of previous exercises\n",
    "\n",
    "class MaxPoolLayer(BaseLayer):\n",
    "    \n",
    "    def __init__(self, neighborhood=(2, 2), stride=(2, 2)):\n",
    "        \"\"\" Max pooling layer.\n",
    "           param: neighborhood: tuple with shape (sp, sq) which denote the kernel size of the pooling operation in \n",
    "           the spatial dimensions\n",
    "           param: stride: tuple with shape (np, nq) which denote the subsampling factor of the pooling operation in\n",
    "           the spacial dimensions\n",
    "        \"\"\"\n",
    "        # TODO: define necessary class variables, for this, have a look at the input and at the forward and backward \n",
    "        # function.\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the result of maxpooling on the input.\n",
    "            param: x (np.ndarray) with shape (b, n_channels, p, q) where b is the batch size, \n",
    "                   n_channels is the number of input channels and p x q is the image size\n",
    "            returns (np.ndarray): the result of max pooling, of shape (b, n_channels, p', q')\n",
    "                   where b is the batch size, n_channels is the number of input channels and \n",
    "                   p' x q' is the new image size reduced by the stride. \n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass of max pooling, think of what you need to store for the backward pass\n",
    "        # (1) store the input tensor shape: TODO\n",
    "        \n",
    "        # (2) calculate the output shape: \n",
    "        # for this, loop through the image size (p,q), the neighborhood (sp, sq), and the stride (np, nq) and compute \n",
    "        # the output of the pooling operation according to:\n",
    "        # (W-F + 2P)/S + 1,\n",
    "        # where in our case padding P is 0, W means image size, and F means neighborhood size\n",
    "        # TODO\n",
    "        \n",
    "        # (3) The complete output shape then is [batch_size, n_channel and the result of (2)]\n",
    "        # TODO\n",
    "        \n",
    "        # (4) initialize the max pooling result array using the output size:\n",
    "        result = #TODO\n",
    "        \n",
    "        # (4) Create an empty dictionary to store the values for the switch operation used in the backward pass\n",
    "        self.switches = # TODO\n",
    "        \n",
    "        # (5) Now, compute the forward pass of max pooling: Loop over the output shape\n",
    "        for this_batch in #...\n",
    "            for this_channel in #...\n",
    "                for y_out in #...\n",
    "                    for x_out in #...\n",
    "                        x_in = #TODO\n",
    "                        input_part = #TODO: get values of x according to current indices and neighborhood size\n",
    "                        max_idx = #TODO: get max index of input part, hint: have a look at np.unravel_index\n",
    "                        #define a shape tuple to access the switch variable\n",
    "                        shape = #TODO\n",
    "                        self.switches[shape] = #TODO\n",
    "                        #assign values to result\n",
    "                        result[shape] = #TODO\n",
    "        #return the result of max pooling                \n",
    "        pass \n",
    "    \n",
    "    def backward(self, error):\n",
    "        \"\"\" Return the gradient with respect to the previous layer.\n",
    "            param: error(np.ndarray): the gradient passed own from the subsequent layer, \n",
    "                   of shape [b, n_channels, p', q'] where b is the batch size, n_channels is the \n",
    "                   number of channels and p' x q' is the image size reduced by the stride\n",
    "            returns (np.ndarray): the gradient w.r.t. the previous layer, of shape [b, n_channels, p, q] \n",
    "                   where b is the batch size, n_channels is the number of input channels to this layer and \n",
    "                   p x q is the image size prior to downsampling.\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass of max pooling\n",
    "        \n",
    "        # (1) initialize a variable to compute the gradient\n",
    "        # TODO\n",
    "        \n",
    "        # (2) Iterate over the switch dictionary: TODO\n",
    "        # Hint: subgradients: the winner takes it all, see description\n",
    "        \n",
    "        #return the gradient w.r.t. the previous layer\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%run Tests/TestMaxPoolLayer.py\n",
    "TestMaxPooling.MaxPooling = MaxPoolLayer\n",
    "TestMaxPooling.FullyConnected = FullyConnectedLayer\n",
    "TestMaxPooling.Flatten = FlattenLayer\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Most successful deep learning models use some regularization techniques intended to decrease the gap between training and test accuracy. The goal is to bias the model towards a model with lower training accuracy but better generalization capability. One prominent technique is dropout. It was for example used in the famous AlexNet network. \n",
    "The idea of this technique is to break dependencies between features by setting random activations to zero during training. This is typically done with a Bernoulli distribution: In each training iteration, the probability for a certain activation to \"drop out\" is $1-p$.\n",
    "The application of dropout shifts the mean of the activations because many elements are set to zero during training. At test time, when no element are dropped out, the mean is different, which can decrease performance. To combat this the \"training mean\" can be restored by multiplying all activations with $p$ at test time.\n",
    " \n",
    "### Inverted dropout\n",
    "The multiplication at test time can be avoided by rewriting the dropout behavior during training. This means that the dropout layer can actually be skipped completely during test time, allowing for faster inference. To this end, the activations are multiplied by $\\frac{1}{p}$ after applying the stochastic function during training. This way, the mean is not changed by the layer and no operation needs to be performed during test time. We will implement this \"inverted dropout version\" in the exercise.\n",
    "\n",
    "\n",
    "### Implementation task\n",
    "In the following, implement the ```DropOut``` layer based on the inverted dropout description above. As usual, check your implementation by running the unittests. Note that dropout operates on each element of the input vector independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/layers/dropout_0.py\n",
    "#----------------------------------\n",
    "# Exercise: Dropout\n",
    "#----------------------------------\n",
    "# The original python file can be reloaded by typing %load src/layers/dropout_0.py in the first line of this cell.\n",
    "# After successfully solving this exercise, type the following command in the first line of this cell:\n",
    "# %%writefile src/layers/dropout.py\n",
    "# This will save the result to a python file, which you will need for the next exercises.\n",
    "\n",
    "from src.base import BaseLayer, Phase\n",
    "from src.layers.initializers import He, Const #your results of previous exercises\n",
    "from src.layers.conv import FlattenLayer, ConvolutionalLayer #your results of previous exercises\n",
    "from src.layers.fully_connected import FullyConnectedLayer #your results of previous exercises\n",
    "from src.layers.pooling import MaxPoolLayer #your results of previous exercises\n",
    "\n",
    "class DropOut(BaseLayer):\n",
    "    \n",
    "    def __init__(self, probability):\n",
    "        \"\"\" DropOut Layer.\n",
    "            param: probability: probability of each individual activation to be set to zero, in range [0, 1]    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Implement initialization        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the layer: Set activations of the input randomly to zero.\n",
    "            param: x (np.ndarray): input\n",
    "            returns (np.ndarray): a new array of the same shape as x, after dropping random elements\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass of the Dropout layer\n",
    "        \n",
    "        #Forward pass for training\n",
    "        if self.phase == Phase.train:\n",
    "            #define a binary mask that applies a random choice [0,1] for each pixel with the probability [1-p,p]\n",
    "            self.mask = #TODO\n",
    "            #compute the inverted dropout: x*mask/p\n",
    "            #TODO: return result of inverted dropout\n",
    "        \n",
    "        #Forward pass for testing\n",
    "        else: \n",
    "            #TODO: return result for test phase\n",
    "    \n",
    "    def backward(self, error):\n",
    "        \"\"\" Backward pass through the layer: Return the gradient with respect to the input.\n",
    "            param: error (np.ndarray): error passed down from the subsequent layer, of the same shape as the \n",
    "                   output of the forward pass\n",
    "            returns (np.ndarray):  gradient with respect to the input, of the same shape as error\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass of the Dropout layer (case: inverted dropout!)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%run Tests/TestConv.py\n",
    "TestConv.Conv = ConvolutionalLayer\n",
    "TestConv.FullyConnected = FullyConnectedLayer\n",
    "TestConv.He = He\n",
    "TestConv.Constant = Const\n",
    "TestConv.Flatten = FlattenLayer\n",
    "\n",
    "%run Tests/TestDropout.py\n",
    "TestDropout.DropOut = DropOut\n",
    "TestDropout.Phase = Phase\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
